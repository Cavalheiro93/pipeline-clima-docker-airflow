"""
Pipeline Completo de Coleta, Tratamento e ConsolidaÃ§Ã£o de Dados ClimÃ¡ticos

Esta DAG integra todo o fluxo de ETL:
1. Agendamento da DAG
1. Dispara a DAG de coleta (`dag_coleta_clima`)
2. Aguarda todos os arquivos da pasta `raw/` estarem disponÃ­veis
3. Dispara a DAG de tratamento (`dag_trata_dados_climaticos`)
4. Aguarda todos os arquivos na pasta `processed/`
5. Dispara a DAG de consolidaÃ§Ã£o (`dag_consolida_dados_climaticos`)

Autor: Caio Cavalheiro
Data de CriaÃ§Ã£o: 20/04/2025
"""

from airflow import DAG
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.sensors.python import PythonSensor
from datetime import datetime, timedelta
import os

# ðŸ” Verifica se todos os arquivos .parquet estÃ£o na pasta RAW
def verificar_arquivos_raw():
    caminho = "/opt/airflow/data/raw"
    arquivos_esperados = {
        "sao_paulo_clima.parquet",
        "salvador_clima.parquet",
        "rio_de_janeiro_clima.parquet",
        "sao_bernardo_clima.parquet"
    }
    return arquivos_esperados.issubset(set(os.listdir(caminho)))

# ðŸ” Verifica se todos os arquivos .parquet estÃ£o na pasta PROCESSED
def verificar_arquivos_processed():
    caminho = "/opt/airflow/data/processed"
    arquivos_esperados = {
        "sao_paulo_clima.parquet",
        "salvador_clima.parquet",
        "rio_de_janeiro_clima.parquet",
        "sao_bernardo_clima.parquet"
    }
    return arquivos_esperados.issubset(set(os.listdir(caminho)))



# âš™ï¸ ConfiguraÃ§Ãµes adicionais: Retry e controle de falhas
default_args = {
    "retries": 3,                                # Tenta executar atÃ© 3 vezes em caso de falha
    "retry_delay": timedelta(minutes=2),         # Intervalo de 2 minutos entre as tentativas
    "email_on_failure": False,                   # NÃ£o envia e-mail em falhas
    "email_on_retry": False                      # NÃ£o envia e-mail em retries
}


#1. ðŸŽ¯ DefiniÃ§Ã£o da DAG principal
with DAG(
    default_args=default_args,
    dag_id="pipeline_completo_clima",
    start_date=datetime(2025, 4, 1),
    schedule_interval="*/5 * * * *",  # Executa a cada 5 minutos
    catchup=False,
    tags=["clima", "encadeado", "pipeline"]
) as dag:

    #2. ðŸš€ Dispara a DAG de coleta
    trigger_coleta = TriggerDagRunOperator(
        task_id="disparar_dag_coleta",
        sla=timedelta(minutes=5),
        trigger_dag_id="dag_coleta_clima"
    )

    #3. ðŸ•µï¸ Sensor: aguarda todos os arquivos na pasta RAW
    aguardar_raw = PythonSensor(
        task_id="aguardar_todos_arquivos_raw",
        python_callable=verificar_arquivos_raw,
        poke_interval=10,
        timeout=300,
        mode="poke"
    )

    #4. ðŸš€ Dispara a DAG de tratamento
    trigger_tratamento = TriggerDagRunOperator(
        task_id="disparar_dag_tratamento",
        sla=timedelta(minutes=5),
        trigger_dag_id="dag_trata_dados_climaticos"
    )

    #5. ðŸ•µï¸ Sensor: aguarda todos os arquivos na pasta PROCESSED
    aguardar_processed = PythonSensor(
        task_id="aguardar_todos_arquivos_processed",
        python_callable=verificar_arquivos_processed,
        poke_interval=10,
        timeout=300,
        mode="poke"
    )

    #6. ðŸš€ Dispara a DAG de consolidaÃ§Ã£o
    trigger_consolidacao = TriggerDagRunOperator(
        task_id="disparar_dag_consolidacao",
        sla=timedelta(minutes=5),
        trigger_dag_id="dag_consolida_dados_climaticos"
    )

    #7. âž• Encadeamento do pipeline
    trigger_coleta >> aguardar_raw >> trigger_tratamento >> aguardar_processed >> trigger_consolidacao
