# ğŸŒ¤ï¸ Projeto 02 - Pipeline de Coleta e Processamento de Dados ClimÃ¡ticos com Airflow + Docker

<br>

## ğŸ“Œ VisÃ£o Geral
Este projeto foi desenvolvido com o objetivo de **aprender e aplicar na prÃ¡tica duas ferramentas amplamente utilizadas no mercado de Engenharia de Dados: o `Apache Airflow` e o `Docker`**.

O Projeto nÃ£o ignora a construÃ§Ã£o de um pipeline real â€” pelo contrÃ¡rio: **foi criado um fluxo completo de ingestÃ£o, tratamento e consolidaÃ§Ã£o de dados climÃ¡ticos via API pÃºblica**, com mÃºltiplas etapas encadeadas, apesar de simples. 

No entanto, **o foco principal foi o aprofundamento no uso do Docker e, principalmente, do Airflow**, testando sensores, operadores, DAGs compostas e prÃ¡ticas comuns de orquestraÃ§Ã£o. 

Esse projeto representou **um mergulho prÃ¡tico no universo dessas ferramentas**, indo alÃ©m do bÃ¡sico e simulando desafios reais.

<br><br>

## ğŸš€ Tecnologias Utilizadas
- `Apache Airflow`: orquestraÃ§Ã£o das tarefas com DAGs, operadores, sensores e encadeamento de processos.

- `Docker`: containerizaÃ§Ã£o completa do ambiente de orquestraÃ§Ã£o com PostgreSQL e Airflow Webserver.

- `API Open-Meteo`: coleta de dados climÃ¡ticos em tempo real via requisiÃ§Ãµes HTTP.

- `Python`: linguagem base para scripts de coleta, tratamento e consolidaÃ§Ã£o de dados.

- `Pandas`: utilizado para manipulaÃ§Ã£o, tratamento e consolidaÃ§Ã£o dos dados em .parquet.

- `Git + GitHub`: controle de versÃ£o e documentaÃ§Ã£o do projeto.

<br><br>

## ğŸ“‚ Estrutura do Projeto

```
â”œâ”€â”€ dags/                           # DAGs do Airflow
â”‚   â”œâ”€â”€ dag_coleta_clima.py
â”‚   â”œâ”€â”€ dag_tratamento.py
â”‚   â”œâ”€â”€ dag_consolidacao.py
â”‚   â””â”€â”€ pipeline_completo_clima.py
â”‚
â”œâ”€â”€ src/                            # Scripts auxiliares (coleta, tratamento, consolidaÃ§Ã£o)
â”‚   â”œâ”€â”€ coleta/
â”‚   â”‚   â”œâ”€â”€ cidades/                # Arquivo JSON com cidades que serÃ£o coletadas
â”‚   â”‚   â”‚   â””â”€â”€ cidades.json
â”‚   â”‚   â””â”€â”€ coleta_api.py
â”‚   â””â”€â”€ processamento/
â”‚       â”œâ”€â”€ tratamento.py
â”‚       â””â”€â”€ consolidacao.py
â”‚
â”œâ”€â”€ data/                           # Dados gerados
â”‚   â”œâ”€â”€ raw/                        # Arquivos brutos coletados da API
â”‚   â”œâ”€â”€ processed/                  # Arquivos tratados
â”‚   â””â”€â”€ final/                      # Arquivo final consolidado
â”‚
â”œâ”€â”€ docker/                         # Arquivos de configuraÃ§Ã£o Docker
â”‚   â””â”€â”€ docker-compose.yaml
â”‚
â”œâ”€â”€ .env                            # VariÃ¡veis de ambiente (API, parÃ¢metros etc.)
â”œâ”€â”€ requirements.txt                # DependÃªncias do projeto
â””â”€â”€ README.md                       # DocumentaÃ§Ã£o do projeto
```

<br><br>

## âš™ï¸ Como Executar o Projeto Localmente
Para rodar este projeto localmente, siga os passos abaixo:

### 1. Clone o repositÃ³rio no seu VSCode via Terminal
```
git clone https://github.com/Cavalheiro93/pipeline-clima-docker-airflow.git

cd pipeline-clima-docker-airflow
```

<br>

### 2. Crie o arquivo `.env`
>ğŸ“„ **.env**  
_Arquivo com variÃ¡veis de ambiente utilizadas no projeto, como a URL base da API climÃ¡tica e os parÃ¢metros desejados (ex: `temperature_2m`)._

Na raiz do projeto, crie um arquivo `.env` com o seguinte conteÃºdo:
```
API_BASE_URL=https://api.open-meteo.com/v1/forecast
PARAMETROS=temperature_2m
```

<br>

### 3. Navegue atÃ© a pasta `docker/`
Certifique-se de ter o Docker instalado. Depois, navegue atÃ© a pasta `docker/`:

```
cd docker
```

<br>

### 4. Crie um usuÃ¡rio no Airflow e Inicialize o banco de dados 
Antes de subir os containers principais, execute os seguintes comandos para:
- Criar um usuÃ¡rio administrador para acessar a interface Web
- Inicializar o banco de dados do Airflow

```
docker compose run airflow-webserver airflow users create \
  --username airflow \
  --password airflow \
  --firstname <seu-nome> \
  --lastname <seu-sobrenome> \
  --role Admin \
  --email <seu-email>
```
```
docker compose run airflow-webserver airflow db init
```

>ğŸ” _Substitua os campos entre `<>` pelas suas informaÃ§Ãµes pessoais.   
Por exemplo: --firstname Joao, --email joao@gmail.com_

> ğŸ§  _Dica: No projeto, criamos o usuÃ¡rio com username `airflow` e senha `airflow` por simplicidade.  
Use essas credenciais para acessar o Airflow em http://localhost:8080 posteriormente._

<br>

### 5. Suba os serviÃ§os principais (Webserver + Scheduler)
Agora sim, suba os containers do Airflow:
```
docker compose up airflow-webserver airflow-scheduler
```
> ğŸ§  _Dica: Para encerrar o Docker, no Terminal use o CTRL+C para interromper_

ğŸ’¡ Se preferir que os containers rodem em segundo plano (sem travar o terminal), adicione a flag `-d` no final do comando:
```
docker compose up -d airflow-webserver airflow-scheduler
```

<br>

### 6. Verifique se tudo estÃ¡ funcionando corretamente
Se os passos anteriores foram seguidos com sucesso, acesse a interface web do Airflow pelo navegador:

ğŸŒ http://localhost:8080


<p align="center">
  <img src="images\image.png" alt="Tela de Login 1" width="40%" />
  <img src="images\image2.png" alt="Tela de Login 2" width="49%" />
</p>

<br><br>

## âš™ï¸ Como o Projeto Funciona (VisÃ£o TÃ©cnica)
Este projeto simula um pipeline de dados completo, automatizado com o Apache Airflow e executado dentro de containers Docker. Abaixo estÃ¡ o fluxo tÃ©cnico da soluÃ§Ã£o:

### â†•ï¸ Fluxo Geral
#### **Coleta de Dados ClimÃ¡ticos**
ResponsÃ¡vel por consumir a API pÃºblica com base nas cidades configuradas em *cidades.json*.

ğŸ“‚ Arquivo principal: `src/coleta/coleta_api.py`  
ğŸ“‚ Cidades: `src/coleta/cidades/cidades.json`

#### **Tratamento dos Dados**
Realiza a limpeza, organizaÃ§Ã£o e transformaÃ§Ã£o dos dados coletados, gerando arquivos no formato *.parquet*.

ğŸ“‚ Script: `src/processamento/tratamento.py`

#### **ConsolidaÃ§Ã£o dos Dados**

Une todos os dados tratados em um Ãºnico DataFrame e gera um arquivo final consolidado.

ğŸ“‚ Script: `src/processamento/consolidacao.py`

#### **OrquestraÃ§Ã£o com Airflow**
Todas as etapas sÃ£o controladas por DAGs que garantem a execuÃ§Ã£o sequencial correta, com sensores monitorando a presenÃ§a dos arquivos.

ğŸ“‚ DAG: `dags/dag_coleta_clima.py`

ğŸ“‚ DAG: `dags/dag_tratamento.py`

ğŸ“‚ DAG: `dags/dag_consolidacao.py`

ğŸ“‚ DAG: `dags/pipeline_completo_clima.py`

<br><br>

## <img src="images/py.png" alt="" width="20"> DAG pipeline_completo_clima.py 

Essa Ã© a DAG que integra todo o pipeline, conectando as trÃªs etapas principais: coleta, tratamento e consolidaÃ§Ã£o dos dados climÃ¡ticos.

### â†•ï¸ Fluxo da DAG

![alt text](images/fluxo_pipeline.png)

### âš™ï¸ Ferramentas do Airflow utilizadas:
`TriggerDagRunOperator`: utilizado para acionar as outras DAGs (coleta, tratamento, consolidaÃ§Ã£o).

`PythonSensor`: responsÃ¡vel por aguardar a presenÃ§a dos arquivos antes de seguir para a prÃ³xima etapa.

`Encadeamento com >>`: garante a ordem de execuÃ§Ã£o correta.

`schedule_interval`: A DAG passa a ser executada automaticamente via UI do Airflow, configurada para rodar a cada 5 minutos.

<br><br>

## Resultado Final

Com todas as DAGs ativadas no Toggle Button, a `pipeline_completo_clima` serÃ¡ executada automaticamente, orquestrando as etapas de coleta, tratamento e consolidaÃ§Ã£o conforme a ordem definida.

Se tudo ocorrer corretamente, na interface do Airflow (aba "Recent Tasks") vocÃª verÃ¡ os cÃ­rculos das tarefas marcados em verde âœ…, indicando sucesso na execuÃ§Ã£o.


![alt text](images/resultado1.png)

ApÃ³s a execuÃ§Ã£o, os arquivos serÃ£o gerados nas seguintes pastas:

![alt text](image.png)

O arquivo `dados_consolidados.parquet` representa a junÃ§Ã£o final dos dados de todas as cidades em um Ãºnico dataset, pronto para anÃ¡lises ou visualizaÃ§Ãµes futuras.
